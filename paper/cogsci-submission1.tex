\documentclass[a4paper, 11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\bibpunct[:]{(}{)}{,}{a}{}{;}

%--------------------
%\usepackage{gb4e}
%\noautomath

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{nicefrac}
%\usepackage{stmaryrd}
%\usepackage{multicol}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{subcaption}
\pgfplotsset{compat=1.3}
\usetikzlibrary{pgfplots.groupplots,decorations.markings}

\newtheoremstyle{Satz}
   {}                      %Space above
   {1em}                   %Space below
   {\normalfont}           %Body font
   {}                      %Indent amount (empty = no indent,
                           %\parindent = para indent)
   {\normalfont}           %Thm head font
   {.}                     %Punctuation after thm head
   {.8em}                  %Space after thm head: " " = normal interword
                           %space; \newline = linebreak
   {\bfseries\thmname{#1}\thmnote{ (#3)}}
                           %Thm head spec (can be left empty, meaning
                           %`normal')

\theoremstyle{Satz}
\newtheorem{example}{Example}

%\newcommand{\mvalueof}[1]{\llbracket#1\rrbracket}
\newcommand{\citeposs}[2][]{\citeauthor{#2}'s (\citeyear[#1]{#2})}
\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}} 

\newcommand{\hl}[1]{\textcolor[rgb]{.8,.33,.0}{#1}}% prints in orange
%\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
%\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
%\newcommand{\sbna}{\exists\lnot\forall}

\definecolor{Red}{RGB}{178,34,34}
\newcommand{\mf}[1]{\textcolor{Red}{[MF: #1]}} 
\newcommand{\tb}[1]{\textcolor[rgb]{.8,.33,.0}{[TB: #1]}}% prints in orange

\usepackage{blkarray}
\usepackage{xspace}

\usepackage{tgtermes}
\renewcommand{\baselinestretch}{1.2}

%%% MF's commands
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\card}[1]{\left \lvert \, #1 \, \right\rvert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\States}{\ensuremath{S}\xspace}		% Set of States
\newcommand{\state}{\ensuremath{s}\xspace}		% single states
\newcommand{\mystate}[1]{\ensuremath{\state_{\text{#1}}}\xspace} %meaningful states
\newcommand{\mylang}[1]{\ensuremath{L_{\text{#1}}}\xspace} %meaningful states
\newcommand{\Messgs}{\ensuremath{M}\xspace}		% Set of Messages
\newcommand{\messg}{\ensuremath{m}\xspace}		% single messages
\newcommand{\mymessg}[1]{\ensuremath{\messg_{\text{#1}}}\xspace} %meaningful messages
\newcommand{\ssome}{\mystate{\ensuremath{\exists\neg\forall}}}
\newcommand{\sall}{\mystate{\ensuremath{\forall}}}
\newcommand{\snone}{\mystate{\ensuremath{\emptyset}}}
\newcommand{\msome}{\mymessg{some}}
\newcommand{\mall}{\mymessg{all}}
\newcommand{\mnone}{\mymessg{none}}
\newcommand{\Lall}{\mylang{all}}
\newcommand{\Lbound}{\mylang{bound}}
\newcommand{\Llack}{\mylang{lack}}
\newcommand{\asome}{\myact{\ensuremath{\exists\neg\forall}}}
\newcommand{\aall}{\myact{\ensuremath{\forall}}}

\definecolor{mygray}{cmyk}{0.35,0.35,0.35,0.35}
\newcommand{\mygray}[1]{{\textcolor{mygray}{#1}}}
%%% 


%--------------------
%
\usepackage{setspace}
\doublespacing
%
%-------------------


\title{Co-evolution of lexical meaning \& pragmatic use}

% \author{%\bf NAME1 and NAME2\\
%     ( -- draft \today --- )
% }


\date{}

\begin{document}

  %% for arrow head placement
  \tikzset{->-/.style={decoration={
  markings,
  mark=at position #1 with {\arrow{>}}},postaction={decorate}}}
  %%% 

\maketitle

\begin{abstract}
  According to standard linguistic theory, the meaning of an utterance is the product of
  conventional semantic meaning and general pragmatic rules on language use. To investigate how
  cultural evolution of language plays out under this picture of the semantics-pragmatics
  interface, we present a game theoretic model of the competition between types of language
  users, each endowed with a selection of lexical representations and a particular pragmatic
  disposition to act on them. Our model traces two evolutionary forces and their interaction:
  (i) pressure towards communicative efficiency and (ii) learning biases during the transfer of
  linguistic knowledge. We illustrate the model based on a case study on scalar
  implicatures. In this case study learning biases that favor simple semantic representations
  can foster the evolution of more sophisticated pragmatic reasoning types and so prevent the
  lexicalization of scalar implicatures.
\end{abstract}

\section{Introduction}\label{sec:introduction}
What is conveyed usually goes beyond what is said. A request for a blanket can be politely
veiled by uttering ``I'm cold;'' temporal succession of events can be communicated by the order
in which conjuncts appear as in ``I traveled to Paris and got married;'' an invitation can be
declined by saying ``I have to work.'' An influential explanation of the relation between the
literal meaning of expressions and what they may convey in context is due to
\citet{grice:1975}, who characterizes pragmatic use and interpretation as a process of mutual
reasoning about rational language use. For instance, under the assumption that the speaker is
cooperative and relevant, ``I have to work'' may be interpreted as providing a reason why the
speaker will not be able to accept an invitation, going beyond its literal meaning. Some of
these enrichments are rather \emph{ad hoc}. Others show striking regularities, such as the use
of ability questions for polite requests (``Could you please \dots?''), or certain enrichments
of lexical meanings such as \emph{and} to convey \emph{and then}.

A particularly productive and well studied class of systematic pragmatic enrichments are scalar
implicatures
\citep{horn:1984,Hirschberg1985:A-Theory-of-Sca,LevinsonPragmatics1983,Geurts2010:Quantity-Implic}. Usually,
the utterance of a sentence like ``I own some of Johnny Cash's albums'' will be taken to mean
that the speaker does not own all of them. This is because, if the speaker instead owned them all, she
could have used the word \emph{all} instead of \emph{some} in his utterance, thereby making a
more informative statement. Scalar implicatures, especially the inference from \emph{some} to
\emph{some but not all}, have been studied extensively, both theoretically
\citep[e.g.,][]{Sauerland2004:Scalar-Implicat,ChierchiaFox2008:The-Grammatical,Rooyvan-RooijJagerde-Jager2012:Explaining-Quan}
as well as experimentally
\citep[e.g.,][]{BottNoveck2004:Some-Utterances,huang+snedeker:2009,GrodnerKlein2010:Some-and-Possib,GoodmanStuhlmuller2013:Knowledge-and-I,DegenTanenhaus2012:Processing-Scal}. While
there is much dispute in this domain about many details, a position endorsed by a clear
majority is that a scalar item like \emph{some} is underspecified to mean \emph{some and maybe
  all} and that the enrichment to \emph{some but not all} is part of some regular process with
roots in pragmatics.

If this majority view is correct, the question arises how such a division of labor between
semantics and pragmatics could have evolved, why it would be so pervasive across natural
languages, and why it is that some expressions systematically draw from it while others semantically conventionalize. Models of language evolution abound. There are simulation-based models studying
populations of communicating agents
\citep{Hurford1989:Biological-Evol,Steels1995:A-Self-Organizi,LenaertsJansen2005:The-Evolutionar,SteelsBelpaeme2005:Coordinating-Pe,BaronchelliPuglisi2008:Cultural-route-,steels:2011,SpikeStadler2016:Minimal-Require}
and there are mathematical models of language evolution, mostly coming from game theory
\citep{lewis:1969,Warneryd1993:Cheap-Talk-Coor,BlumeKim1993:Evolutionary-St,nowak+krakauer:1999,Huttegger2007:Evolution-and-t,Skyrms2010:Signals}. Much
of this work has focused on explaining basic properties such as compositionality and
combinatoriality
\citep[e.g.,][]{Batali1998:Computational-S,nowak+krakauer:1999,nowak+etal:2000,KirbyHurford2002:The-Emergence-o,kirby:2002,SmithKirby2003:Iterated-Learni,Gong2007:Language-Evolut,kirby+etal:2015,verhoef+etal:2014,Franke2015:Proto-Syntax},
but little attention has been paid to the interaction between conventional meaning and
pragmatic use. What is more, many mathematical models explain evolved meaning as a regularity
in the overt behavior of agents, abstracting from complex interactions between semantic
representations and pragmatic use. In contrast, we will here look at language users with a
richer cognitive make-up.


We spell out a model of the co-evolution of conventional meaning and pragmatic reasoning.
The objects of replication and selection are pairs consisting of a set of lexical
meanings and a manner of pragmatic behavior. The latter are represented using probabilistic
models of pragmatic language use
\citep{frank+goodman:2012,FrankeJager2015:Probabilistic-p,GoodmanFrank2016:Pragmatic-Langu}. Replication
and selection are described by the \emph{replicator mutator dynamic}, a general and established
model of evolutionary change in large and homogeneous populations
\citep{Hofbauer1985:The-Selection-M,nowak+etal:2000,NowakKomarova2001:Evolution-of-Un,hofbauer+sigmund:2003,Nowak2006:Evolutionary-Dy}. This
approach allows us to study the interaction between (i) pressure towards communicative
efficiency and (ii) infidelity in the transmission of linguistic knowledge, caused by factors
such as inductive learning biases and sparse learning data. Considering transmission of
linguistic knowledge is important because neither semantic meanings nor pragmatic usage
patterns are directly observable. Instead, language learners have to infer these unobservables
from the observable behavior in which they result. We formalize this process as a form of
Bayesian inference. Our approach thereby contains as a special case a well-understood model of
iterated Bayesian learning \citep{griffiths+kalish:2007}, but combines it with functional
selection, here formalized as the most versatile dynamic from evolutionary game theory, the
replicator dynamic \citep{TaylorJonker1978:Evolutionary-St}. Section~\ref{sec:model} introduces
this model.

Section~\ref{sec:si-case-study} applies this model to a case study on scalar implicatures. We
discuss a setting in which the majority view of underspecified lexical meanings and systematic pragmatic
enrichments emerges if selection and transmission infidelity are combined. In particular, we
show that inductive learning biases of Bayesian learners that favor simpler lexical meanings
can prevent the lexicalization of scalar inferences and lead to the emergence of Gricean-like
pragmatic reasoning types. Results of this case study are critically assessed in the light of
the assumptions that feed our model in Section~\ref{sec:discussion}.

% We see the main contribution of this work as conceptual and technical, not as a definite answer
% to the question why scalar implicatures emerge. It rather demonstrates how current
% probabilistic cognitive modeling of language use and evolutionary modeling can be fruitfully
% combined to study the co-evolution of semantics and pragmatics side-by-side. Reversely, the
% approach taken here may be seen as a first step towards giving an evolutionary rationale for
% empirically successful probabilistic models of language use that embrace the majority view of
% the division of labor between semantics and pragmatics. Section~\ref{sec:discussion} elaborates
% on these points.

\section{A model of co-evolving lexical representations and pragmatic behavior}
\label{sec:model}

\subsection{Communicative efficiency and learnability}

The emergence and change of linguistic structure is influenced by many intertwined
factors. These range from biological and socio-ecological to cultural ones \citep{benz+etal:2005b,steels:2011,tamariz+kirby:2016}. Social and ecological pressures determine communicative needs, while
biology determines the architecture that enables and constrains the means by which they can be
fulfilled. In the following, our focus lies on cultural aspects, wherein processes of
linguistic change are viewed as shaped by language use and its transmission, i.e., as a result
of a process of cultural evolution
\citep{Pagel2009:Human-Language-,ThompsonKirby2016:Culture-Shapes-}.

The idea that language is an adaptation to serve a communicative function is fundamental to
many synchronic and diachronic analyses at least since \citeposs{zipf:1949} explanation of word
frequency rankings as a result of competing hearer and speaker preferences \citep[e.g.,
in][]{martinet:1962, horn:1984,jaeger+vRooij:2007,jaeger:2007,
  piantadosi:2014,kirby+etal:2015}. If processes of selection, such as conditional imitation or
reinforcement, favor more communicatively efficient types of behavior, languages are driven
towards semantic expressivity \citep[e.g.,][]{nowak+krakauer:1999,Skyrms2010:Signals}. But
pressure towards communicative efficiency is not the only force that shapes
language. Learnability is another, as natural languages need to be learnable to survive their
faithful transmission across generations. Furthermore, even small learning biases implicit in acquisition
can build up and have quite striking effects on an evolving language in a process of iterated
learning
\citep{KirbyHurford2002:The-Emergence-o,SmithKirby2003:Iterated-Learni,kirby+etal:2014}.

While natural languages are pressured for both communicative efficiency and learnability, these forces may
pull in opposite directions. Their opposition becomes particularly clear when considering the
extreme (cf. \citealt{kemp+regier:2012,kirby+etal:2015}). A language consisting of a single
form-meaning association is easy to learn but lacking in expressivity. Conversely, a language
that lexicalizes a distinct form for a large number of different meanings is highly expressive
but challenging to acquire.

\subsection{The replicator mutator dynamic}

An elegant formal approach to capture the interaction between communicative efficiency and learnability is
the \emph{replicator mutator dynamic}
\citep{Hofbauer1985:The-Selection-M,nowak+etal:2000,NowakKomarova2001:Evolution-of-Un,hofbauer+sigmund:2003,Nowak2006:Evolutionary-Dy}. In
its simplest, discrete-time formulation, the RMD defines the frequency $x'_i$ of each type $i$
in a population at the next time step as a function of: (i) the frequency $x_i$ of each type
$i$ before the update step, (ii) the fitness $f_i$ of each type $i$ before the update, and
(iii) the probability $Q_{ji}$ that an agent who wants to imitate, adopt, or learn the type of
an agent with type $j$ ends up acquiring type $i$:
\begin{align}
  \label{eq:RMD_discrete}
  x'_i = \sum_j Q_{ji} \frac{x_jf_j}{\sum_k x_k f_k}\,.
\end{align}
The RMD consists of two components: fitness-based selection and transmission biases. This
becomes most transparent when we consider an equivalent formulation in terms of a step-wise
application of the discrete-time replicator dynamic \citep{TaylorJonker1978:Evolutionary-St} on the initial population vector $\vec{x}$
and its subsequent multiplication with a mutation matrix $Q$:
\begin{align}
  \label{eq:RMD_discrete_recast}
  x'_i & = (\text{M}(\text{RD}(\vec{x})))_i\,,
\end{align}
where
\begin{align*}
      \left ( \text{RD}(\vec{x}) \right )_i 
         = \frac{x_i f_i}{\sum_k x_k f_k}
 \ \ \ \ \text{and} \ \ \ \ 
  (\text{M}(\vec{x}))_i = (\vec{x} \cdot Q)_i = \left ( \sum_j
          x_j Q_{ji} \right)_i\,.
\end{align*}
If the transmission matrix $Q$ is trivial in the sense that $Q_{ji}=1$ whenever $j=i$, the
dynamic reduces to the replicator dynamic. The replicator dynamic is a model of fitness-based
selection in which the relative frequency of type $i$ will increase with a gradient
proportional to its average fitness in the population. This dynamic is popular and
versatile because it can be derived from many abstract processes of biological and cultural
transmission and selection \citep[for overview and several derivations
see][]{Sandholm2010:Population-Game}, including conditional imitation
\citep[e.g.,][]{Helbing1996:A-Stochastic-Be,Schlag1998:Why-Imitate-and} or reinforcement
learning \citep[e.g.,][]{BorgersSarin997:Learning-Throug,Beggs2005:On-the-Converge}. If fitness
$f_i$ is the same for all types $i$, the replicator step is the identity map
$ \left ( \text{RD}(\vec{x}) \right )_i = x_i$ and the dynamic reduces to a process of
iteration of the transmission bias encoded in $Q$. In this way, the process in
(\ref{eq:RMD_discrete}), equivalently (\ref{eq:RMD_discrete_recast}), contains a model of
iterated learning \citep{griffiths+kalish:2007}. 

\begin{figure}[t]
  \centering
  %% notice that the figure is hacked, because somehow the curve does not come out right under
  %% PGFplots; I don't know why!?!?
  

    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \begin{tikzpicture}
      \begin{axis}[xlabel=$x$, ylabel = $x'$,
                   grid=major, width=6.85cm, height = 5.5cm,
                   legend style = {legend pos = north west},
                   ymin=0,ymax=1, xmin=0,xmax=1]

      \addplot[smooth, color=blue] {x^2 / (3*x^2 - 4*x + 2) + 0.019 * x};
      \addplot[smooth, color=green] {.8*x + .1};

      \addplot[smooth, color=red] {(.9*x^2 - .2*x +.2)/(3*x^2 - 4*x + 2)};

      \addplot[smooth, color=gray] {x};

      \legend{RD, M, RMD}

      \end{axis}
    \end{tikzpicture}


        \caption{Update functions: the population state $x$ is mapped onto $x'$ in one update step.}
        \label{fig:Updates_RMS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}    
      \centering
    \begin{tikzpicture}[x=120]
      \node[draw=black, circle, fill = black, minimum size = 0.25cm]
      (0) at (0,0) {};

      \node[draw=black, circle, fill = black, minimum size = 0.25cm]
      (1) at (1,0) {};

      \draw[-, thick] (0) -- (1);

      \node[draw=black, circle, fill = white, minimum size = 0.25cm, thick]
      (mid) at (2/3,0) {};

      \node[]  (0.label) at (0,-0.5) {0};

      \node[]  (1.label) at (1,-0.5) {1};

      \node[]  (label) at (0.5,-0.5) {$x$};

      \node[]  (label) at (0.5,0.55) {RD};

      \draw[->-=0.8,very thick] (mid) -- (0);

      \draw[->-=0.8,very thick] (mid) -- (1);

    \end{tikzpicture}
    
    \vspace*{0.35cm}

    \begin{tikzpicture}[x=120]

      \node (0) at (-0.05,0) {};

      \node (1) at (1.05,0) {};

      \draw[-, thick] (0) -- (1);

      \node[draw=black, circle, fill = black, minimum size = 0.25cm, thick]
      (mid) at (0.5,0) {};

      \node[]  (0.label) at (0,-0.3) {0};

      \node[]  (1.label) at (1,-0.3) {1};

      \node[]  (label) at (0.5,-0.5) {$x$};

      \node[]  (label) at (0.5,0.55) {M};

      \draw[->-=0.5,very thick] (0) -- (mid);

      \draw[->-=0.5,very thick] (1) -- (mid);

    \end{tikzpicture}

    \vspace*{0.35cm}

    \begin{tikzpicture}[x=120]

      \node (0) at (-0.05,0) {};

      \node (1) at (1.05,0) {};

      \node[draw=black, circle, fill = black, minimum size = 0.25cm]
      (S0) at (0.121,0) {};

      \node[draw=black, circle, fill = black, minimum size = 0.25cm]
      (S1) at (0.903,0) {};

      \draw[-, thick] (0) -- (1);

      \node[draw=black, circle, fill = white, minimum size = 0.25cm, thick]
      (mid) at (0.609,0) {};

      \node[]  (0.label) at (0,-0.3) {0};

      \node[]  (1.label) at (1,-0.3) {1};

      \node[]  (label) at (0.5,-0.5) {$x$};

      \node[]  (label) at (0.5,0.55) {RMD};

      \draw[->-=0.5,very thick] (mid) -- (S0);

      \draw[->-=0.5,very thick] (mid) -- (S1);

      \draw[->-=0.5,very thick] (0) -- (S0);

      \draw[->-=0.5,very thick] (1) -- (S1);

    \end{tikzpicture}
        


    \caption{Phase portraits for RD, M and RMD: unstable rest points are hollow, attractors are
      solid.}
        \label{fig:Phase_RD}
    \end{subfigure}

  \caption{Example.}
  \label{fig:Example_RMD}
\end{figure}

\begin{example}
  Consider a simple and abstract coordination game. Agents are of two types: positive or negative. If
  agents of different type play with each other, they obtain a payoff of 0. If negative meets
  negative, each receives a payoff of 1. If positive meets positive, they get a payoff of 2. A
  population state is completely characterized by the proportion $x$ of negatives. The fitness
  of negatives in population state $x$ is $f_n(x) = x$, that of positives is $f_p(x) = 2-2x$.
  The average fitness is $\Phi(x) = x f_n(x) + (1-x) f_p(x) = 3x^2 - 4x + 2$. Without mutation,
  the replicator dynamic will then update $x$ to $RD(x) = \nicefrac{f(x)x}{\Phi(x)} = \nicefrac{x^2}{\Phi(x)}$. The update
  function $RD(x)$ of the replicator step is plotted in Fig.~\ref{fig:Updates_RMS} as the
  blue line. Rest points, for which $RD(x)=x$, are at $x=0$, $x=1$ and $x= \nicefrac{2}{3}$.
  The former are attractors as nearby points converge to them. Points near $x=\nicefrac{2}{3}$
  move towards 0 or 1. This is schematically pictured in the topmost phase portrait in
  Fig.~\ref{fig:Phase_RD}.

  Adding mutation changes the dynamic and its rest points. Let us assume that $Q_{ji} = .9$
  when $j=i$. The update effect of mutation alone is $M(x) = .9 x + .1 (1-x) = .8x + .1$ and is
  plotted as the linear green line in Fig.~\ref{fig:Updates_RMS}. It has only one stable rest
  point at $x = 0.5$ (see Fig.~\ref{fig:Phase_RD}). If we first take the replicator step and
  then the mutation step in sequence, we obtain the replicator mutator dynamic
  $RMD(x) = M(RD(x)) = \nicefrac{.9x^2 - .2x +.2}{3x^2 - 4x + 2}$, which is plotted in red in
  Fig.~\ref{fig:Updates_RMS}. The rest points are at $x=.121$, $x=.903$ and $x=.609$. The
  former two are attractors (see Fig.~\ref{fig:Phase_RD}).
\end{example}




\subsection{Fitness \& learnability of lexical meanings \& pragmatic strategies}
\label{sec:fitn--learn}


Moving beyond an abstract example, our goal is to apply the RMD to investigate the co-evolution
of lexical representations and pragmatic behavior. To do so, we need to fix three things: (i) what the
relevant types are, (ii) how fitness derives from communicative success and (iii) how the
mutation matrix $Q$ is computed. These issues are addressed, one by one, in the following.

\subsubsection{Types: Lexica and pragmatic strategies}
\label{sec:languages+use}

Types are what evolution operates on. They define an agent's fitness, usually through a payoff
accrued in single interactions with other agents. Often types can be identified as the possible
acts in a game, e.g., either cooperating or defecting in a prisoner's dilemma. In other cases,
they may be thought of as general properties of an agent that influences her fitness, such as
being positive or negative in our previous example (whatever that means). For our present
purposes, types are identified more concretely by specific assumptions about their cognitive
make-up. Since we are interested in the evolutionary competition between different lexical
representations and ways of using them in communication, a type is here defined as a pair consisting
of a lexicon and a pragmatic strategy of language use.

Agents play signaling games, in which the speaker wants to communicate a world state $s$ with a
message $m$ to a hearer who receives $m$ but does not know $s$
\citep[e.g.][]{lewis:1969,Skyrms2010:Signals}. A lexicon associates each message with a
(possibly fuzzy) set of states. A pragmatic behavior specifies a probabilistic speaker
rule (a probabilistic choice of message for each state) and a probabilistic hearer rule (a
probabilistic choice of state for each message) given a lexicon. There are many ways of making these general
notions more concrete. Here is what we will assume in the remainder of this paper.

Lexica codify the truth-conditions of expressions. A convenient way to represent lexica is by
$(\card{\States}, \card{\Messgs})$-Boolean matrices, where $\States$ is a set of states
(meanings) and $M$ a set of messages (forms available in the language). For example, suppose
that there are two relevant world states $\States = \set{\ssome, \sall}$. In state $\ssome$
Chris owns some but not all of Johnny Cash's albums while in $\sall$ Chris owns them
all. Suppose that there are two messages $\Messgs = \set{\msome, \mall}$ where $\msome$ is
short for a sentence like \emph{Chris owns some of Johnny Cash's albums} and $\mall$ for the
same sentence with \emph{some} replaced by \emph{all}.  Lexica for this case would assign a
Boolean truth value, either $0$ for false or $1$ for true, to each state-message pair. The
following two lexica are minimal examples for the distinction between a lexicalized upper-bound
for \emph{some} in $\Lbound$ and the widely assumed logical semantics with only a lower-bound
in $\Llack$.

\begin{align*}
  \Lbound & = \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & 1 & 0 \\
      \mygray{\sall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} &
  \Llack & = \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & 1 & 0 \\
      \mygray{\sall}  & 1 & 1 \\
    \end{block}
  \end{blockarray}
\end{align*}

Pragmatic strategies define dispositions to produce and interpret messages given a lexicon. We
distinguish between two kinds of pragmatic strategies. {\em Literal interlocutors} produce and
interpret messages literally, being guided only by their lexica. {\em Pragmatic interlocutors}
instead engage in mutual reasoning to inform their choices. Recent probabilistic models of
rational language use
\citep{frank+goodman:2012,FrankeJager2015:Probabilistic-p,GoodmanFrank2016:Pragmatic-Langu}
capture different types of pragmatic behavior in a reasoning hierarchy. The hierarchy's bottom,
level $0$, corresponds to literal language use, as in Equations~(\ref{h:level0}) and
(\ref{s:level0}). Pragmatic language users of level $n + 1$ act (approximately) rational with
respect to level-$n$ behavior of their interlocutors, as in Equations~(\ref{h:leveln}) and
(\ref{s:leveln}).
\begin{alignat}{3}
&H_{0}(s \mid m;L) &&\propto pr(s) L_{[s,m]} \label{h:level0}\\
&S_{0}(m \mid s;L) &&\propto \exp(\lambda \; L_{[s,m]}) \label{s:level0}\\
&H_{n+1}(s|m;L) &&\propto pr(s) S_{n}(m|s;L) \label{h:leveln}\\
&S_{n+1}(m|s;L) &&\propto  \exp(\lambda \; H_{n}(s|m;L)) \label{s:leveln}
\end{alignat}

According to (\ref{h:level0}), a literal hearer's interpretation of a message depends on her
lexicon and her prior over states, $pr \in \Delta(S)$, which is here assumed flat for
simplicity. Literal interpreters thereby choose an arbitrary true interpretation for each
message according to their lexicon. Pragmatic hearers, defined in (\ref{h:leveln}), instead use Bayes' rule to weigh
interpretations based on a conjecture about speaker behavior. Speaker behavior is regulated by
a soft-max parameter $\lambda \geq 0$ \citep{luce:1959,sutton+barto:1998}. As
$\lambda$ increases, choices approximate strict maximization of expected utilities. Expected
utility of a message $\messg$ in state $\state$ for a level $n+1$ speaker is here defined as
$H_{n}(s|m;L)$, the probability that the hearer will assign to or choose the correct
meaning. For literal speakers, utility only tracks truthfulness. Literal speakers choose any
true message with equal probability but may send false messages as well with a probability
dependent on $\lambda$.

The following examples illustrate these behaviors. A literal interpreter with lexicon $\Lbound$ assigns $\ssome$
a probability of $H_0(\ssome \mid \msome; \Lbound) = 1$ after hearing $\msome$, while a literal
interpreter with $\Llack$ has $H_0(\ssome \mid \msome; \Llack) = 0.5$:
\begin{align*}
  H_0(\cdot \mid \cdot, \Lbound) & = \begin{blockarray}{lcc}
    & \mygray{\ssome} & \mygray{\sall} \\
    \begin{block}{l[cc]}
      \mygray{\msome} & 1 & 0 \\
      \mygray{\mall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} &
  H_0(\cdot \mid \cdot, \Llack) & = \begin{blockarray}{lcc}
    & \mygray{\ssome} & \mygray{\sall} \\
    \begin{block}{l[cc]}
      \mygray{\msome} & .5 & .5 \\
      \mygray{\mall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} 
\end{align*}
By contrast, pragmatic hearers of level 1 have the following interpretative behavior for $\lambda = 1$:
\begin{align*}
  H_1(\cdot \mid \cdot, \Lbound) & \approx \begin{blockarray}{lcc}
    & \mygray{\ssome} & \mygray{\sall} \\
    \begin{block}{l[cc]}
      \mygray{\msome} & .73 & .27 \\
      \mygray{\mall}  & .27 & .73 \\
    \end{block}
  \end{blockarray} &
  H_1(\cdot \mid \cdot, \Llack) & \approx \begin{blockarray}{lcc}
    & \mygray{\ssome} & \mygray{\sall} \\
    \begin{block}{l[cc]}
      \mygray{\msome} & .59 & .41 \\
      \mygray{\mall}  & .35 & .65 \\
    \end{block}
  \end{blockarray} 
\end{align*}
This is the outcome of reasoning about their level-$0$ speaker counterparts with $\lambda = 1$:
\begin{align*}
  S_0(\cdot \mid \cdot, \Lbound) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & .73 & .27 \\
      \mygray{\sall}  & .27 & .73 \\
    \end{block}
  \end{blockarray} &
  S_0(\cdot \mid \cdot, \Llack) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & .73 & .27 \\
      \mygray{\sall}  & .5 & .5 \\
    \end{block}
  \end{blockarray} 
\end{align*}
With low $\lambda$ speakers choose true messages with more slack. Reasoning over this behavior therefore also results in a weaker association of messages with only true states in receivers, but also in a slightly stronger association of $\msome$ with $\ssome$ over $\sall$ for $\Llack$ users because they reason that $S_0(\msome | \ssome;\Llack) > S_0(\msome | \sall;\Llack)$. For $\lambda = 20$, there will be less slack in literal speaker behavior:
\begin{align*}
  S_0(\cdot \mid \cdot, \Lbound) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & 1 & 0 \\
      \mygray{\sall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} &
  S_0(\cdot \mid \cdot, \Llack) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} &  1 & 0 \\
      \mygray{\sall}  & .5 & .5 \\
    \end{block}
  \end{blockarray} 
\end{align*}
And accordingly less slack in level $1$ pragmatic interpretation:
\begin{align*}
  H_1(\cdot \mid \cdot, \Lbound) & \approx \begin{blockarray}{lcc}
    & \mygray{\ssome} & \mygray{\sall} \\
    \begin{block}{l[cc]}
      \mygray{\msome} & 1 & 0 \\
      \mygray{\mall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} &
  H_1(\cdot \mid \cdot, \Llack) & \approx \begin{blockarray}{lcc}
    & \mygray{\ssome} & \mygray{\sall} \\
    \begin{block}{l[cc]}
      \mygray{\msome} & 0.67 & 0.33 \\
      \mygray{\mall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} 
\end{align*}
Lastly, turning to types that have no bearing on the choices of hearers of level 1, with $\lambda = 1$ pragmatic speakers of level 1 have:
\begin{align*}
  S_1(\cdot \mid \cdot, \Lbound) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & .73 & .27 \\
      \mygray{\sall}  & .27 & .73 \\
    \end{block}
  \end{blockarray} &
  S_1(\cdot \mid \cdot, \Llack) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & .62 & .38 \\
      \mygray{\sall}  & .38 & .62 \\
    \end{block}
  \end{blockarray} 
\end{align*}
For $\lambda = 20$, pragmatic speaker behavior of level 1 is instead as follows:
\begin{align*}
  S_1(\cdot \mid \cdot, \Lbound) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & 1 & 0 \\
      \mygray{\sall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} &
  S_1(\cdot \mid \cdot, \Llack) & \approx \begin{blockarray}{lcc}
    & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[cc]}
      \mygray{\ssome} & 1 & 0 \\
      \mygray{\sall}  & 0 & 1 \\
    \end{block}
  \end{blockarray} 
\end{align*}







It is particularly important to note that pragmatic agents of level 1 using $\Llack$ associate
$\msome$ preferentially with $\ssome$, in contrast to their literal counterparts of level
0. This association is not perfect, and usually less strong than what agents with a lexicalized
upper bound in $\Lbound$ can achieve, even without pragmatic reasoning. Higher order reasoning
beyond level $1$ leads to stronger associations of $\msome$ and $\ssome$ also for the
hearer. Still, the case study presented in Section~\ref{sec:si-case-study} will consider
speaker and hearer behavior at levels 0 and 1, as the latter are the simplest pragmatic
reasoning types which show a tendency to communicatively attuned pragmatic enrichment. Using
only level 1 reasoning types and possibly small $\lambda$ is therefore a conservative choice
that works against the fitness-based selection of pragmatic language use for a notion of
fitness defined as communicative success, which is introduced next.

\subsubsection{Fitness \& fitness-based selection based on communicative success}\label{sec:expressivity}

Under the replicator dynamic the proportion of type $i$ in a population will increase or
decrease as a function of its relative fitness $f_i$. In the context of language evolution,
fitness is usually associated with the ability to successfully communicate
with other language users from the same population
\citep[e.g.,][]{nowak+krakauer:1999,nowak+etal:2000, nowak+etal:2002}. Under a biological
interpretation the assumption is that organisms have a higher chance of survival and
reproduction if they are able to share and receive useful information via communication with
peers. Under a cultural interpretation the picture is that agents themselves strive towards
communicative success and therefore occasionally adapt or revise their behavior to achieve
higher communicative success (see \citealt[\S3.3]{benz+etal:2005b} for discussion).

The replicator equation gives us the means to make the ensuing dynamic precise, without
necessarily committing to a biological or cultural interpretation. As above, the proportion of
types in a given population is codified in a vector $\vec{x}$, where $x_i$ is the proportion of
type $i$. The fitness of type $i$ is its average expected communicative success, or
\emph{expected utility} (EU), given the frequencies of types in the current population:
\begin{align*}
  f_i = \sum_j x_j \text{EU}(t_i,t_j)\,.
\end{align*}
In signaling games, the expected utility $\text{EU}(t_i,t_j)$ for type $i$ when communicating with type $j$ is the
average success of $i$ when talking or listening to $j$. If agents are speakers half of the
time, this yields:
\begin{align*}
  \text{EU}(t_i,t_j) = \nicefrac{1}{2} \, \text{EU}_S(t_i,t_j) + \nicefrac{1}{2} \, \text{EU}_H(t_i,t_j)\,,
\end{align*}
where $\text{EU}_S(t_i,t_j)$ and $\text{EU}_H(t_i,t_j)$ are the expected utilities for $i$ as a
speaker and as a hearer when communicating with $j$, defined as follows, where $n_i$ and $n_j$
are type $i$'s and type $j$'s pragmatic reasoning types and $L_i$ and $L_j$ are their lexica:
\begin{flalign*}
  & \text{EU}_S(t_i,t_j)  = \sum_s P(s)\sum_m S_{n_i}(m \mid s;L_i) \sum_{s'} H_{n_j}(s' \mid m;L_j)
  \delta(s,s') \,, \\
 & \text{EU}_H(t_i,t_j)  = \text{EU}_S(t_j,t_i)\,.
\end{flalign*}
As usual, $\delta(s,s') = 1$ iff $s = s'$ and $0$ otherwise.

\subsubsection{Learnability}
\label{sec:learnability}

Languages are shaped not only by functionalist forces towards greater communicative success. Another
important factor is the fidelity with which linguistic knowledge is transmitted. Among others,
linguistic production can be prone to errors, states or messages may be perceived incorrectly,
and multiple languages may be compatible with the data learners are exposed to. These sources
of uncertainty introduce variation in the transmission of linguistic knowledge from one
generation to the next. In particular, learning biases in the iterated transmission process can
influence language evolution substantially.

In biological evolution, where types are expressed genetically, transmission infidelity comes
into the picture through infrequent and mostly random genetic mutations. However, an agent's
lexicon and pragmatic reasoning behavior is likely not inherited genetically. They need to be
learned from observation. Concretely, when agents of type $j$ want to adopt or imitate the
linguistic behavior of type $i$, they observe the overt linguistic behavior of type $i$ and
need to infer the covert type that most likely produced the observed behavior. Iterated learning
is a process in which languages are learned repeatedly from the observation of linguistic
behavior of agents who have themselves acquired the language from observation and inference. In
the simplest case there is a single teacher and a single learner. After sufficient training the
learner becomes a teacher and produces behavior that serves as input for a new learner. Due to
the pressure towards learnability it exerts, iterated learning alone generally leads to simpler
and more regular languages (see \citealt{kirby+etal:2014} and \citealt{tamariz+kirby:2016} for
recent surveys).

Following \citet{griffiths+kalish:2007} we model language acquisition as a process of Bayesian
inference in which learners combine the likelihood of a type producing the witnessed learning
input with prior inductive biases. In a Bayesian setting these biases can be codified in a prior
$P \in \Delta(T)$, which reflects the amount of data a learner requires to faithfully acquire
the language of the teacher \citep[cf.][450]{griffiths+kalish:2007}. The extent of the prior's
influence has been shown to heavily depend on the learning strategy assumed to underlie the
inference process. On the one hand, early simulation results suggested that weak biases could
be magnified by exposing learners to only small data samples
\citep[e.g. in][]{brighton:2002}. On the other, \citeposs{griffiths+kalish:2007} mathematical
characterization showed that iterated learning converges to the prior in the limit, i.e., that
the resulting distribution over languages corresponds to the learners' prior distribution and
is not influenced by the amount of input given to them. This difference in predictions can be
traced back to differences in the selection of hypotheses from the posterior. Griffith \&
Kalish's convergence to the prior holds for learners that sample from the posterior. More
deterministic strategies such as the adoption of the type with the highest posterior
probability, so-called {\it maximum a posterior estimation} (MAP), increase the influence of
both the prior and the data \citep{griffiths+kalish:2007,kirby+etal:2007}. In the following, we
use a parameter $l\ge1$ to modulate between posterior sampling and the MAP strategy. When
$l = 1$ learners sample from the posterior. The learners' propensity to maximize the posterior
grows as $l$ increases.

Let $D$ be the set of possible data that learners may be exposed to. This set $D$ contains all
sequences of state-message pairs of length $k$, e.g.,
$\tuple{\tuple{s_1,m_1},\dots , \tuple{s_k,m_k}}$. As $k$ increases, learners have more data to base their inference on and so tend to
recover the true types that generated a given sequence with higher probability. The mutation
matrix $Q$ of the replicator mutator dynamic in (\ref{eq:RMD_discrete}) can then be defined as
follows: $Q_{ji}$ is the probability that a learner acquires type $i$ when learning from an
agent of type $j$. The learner observes a length-$k$ sequence $d$ of state-message pairs, but
the probability $P(d \mid t_j)$ with which sequence $d = \tuple{\tuple{s_1,m_1},\dots , \tuple{s_k,m_k}}$ is observed depends on type $j$'s
linguistic behavior:
\begin{align*}
  P(d = \tuple{\tuple{s_1,m_1},\dots , \tuple{s_k,m_k}} \mid t_j) = \prod_{i = 1}^k S_{n_j}(m_i
  \mid s_i; L_{j})\,,
\end{align*}
where, as before, $n_j$ is $j$'s pragmatic reasoning type and $L_j$ is $j$'s lexicon. For a
given observation $d$, the probability of acquiring type $i$ is $F(t_i \mid d)$, so that:
\begin{flalign*}
  Q_{ji} \propto \sum_{d \in D} P(d \mid t_j) F(t_i \mid d)\,.
\end{flalign*}
The acquisition probability $F(t_i \mid
d)$ given datum $d$ is obtained by probability matching $l = 1$ or a tendency towards choosing
the most likely type $l > 1$ from the posterior distribution $P(\cdot \mid d)$ over types given
the data, which is calculated by Bayes' rule:
\begin{flalign*}
  & F(t_i \mid d) \propto P(t_i \mid d)^l \; \text{ and }\\
  & P(t_i \mid d) \propto P(t_i) P(d \mid t_i)\,.
\end{flalign*}


\subsection{Model summary}

Communicative success and learnability are central to the cultural evolution of language. These
components can be modelled, respectively, as replication based on a measure of fitness in terms
of communicative efficiency and iterated Bayesian learning. Their interaction is described by
the discrete time replicator mutator dynamic in (\ref{eq:RMD_discrete}), repeated here:
\begin{align*}
  x'_i = \sum_j Q_{ji} \frac{x_jf_j}{\sum_k x_k f_k}\,.
\end{align*}
This equation defines the frequency $x'_i$ of type $i$ at the next time step, based on its
frequency $x_i$ before the step, its fitness $f_i$, and the probability that a learner infers
$i$ when observing the behavior of a type-$j$ agent. Fitness-based selection is here thought of
not as biological (fitness as expected relative number of offspring) but cultural (fitness as
likelihood of being imitated or repeated) evolution, since the types that the dynamic operates
on are pairs consisting of a lexicon and a pragmatic use pattern. A type's communicative success depends
on how well it communicates within a population while its learnability depends on the
fidelity by which it is inferred by new generations of learners. The learners' task is
consequently to perform a joint inference over types of linguistic behavior and lexical
meaning.

The model has three parameters: $\lambda$ regulates the degree to which speakers
choose messages that appear optimal from the point of view of the agent's own utility measure
(which may be unrelated to the expected utility when communicating with a given population);
$k$ is length of observations for each language learner; $l$ regulates where the learners'
inference behavior lies on a spectrum from probability matching to acquisition of the most likely
parent type.


\section{Case study: scalar implicatures}
\label{sec:si-case-study}

The model of the previous section formalizes the evolutionary competition between different
sets of lexical representations and ways of using them. This section looks at a case study on
scalar implicatures. It engages in a formal thought experiment to address the
question: if a population of language users could freely combine different lexica with
different pragmatic strategies, what are conditions under which the majority view of scalar
implicatures could have evolved?

Recall that the majority view is that scalar implicatures are non-lexicalized pragmatic
enrichments. Scalar implicature triggers like \emph{some}, \emph{warm} or \emph{may} are
semantically weak expressions for which logically stronger expressions are salient, e.g.,
\emph{all}, \emph{hot} or \emph{must}. For instance, {\em some} is entailed by {\em all}. If
the sentence ``Chris owns all of Johnny Cash's albums'' is true, then ``Chris owns some of
Johnny Cash's albums'' is also true. However, while weaker expressions such as {\em some} are
truth-conditionally compatible with stronger alternatives such as {\em all}, this is not what
their use is normally taken to convey. Instead, the use of a less informative expression when a
more informative one could have been used can license a defeasible inference that stronger
alternatives do not hold (cf. \citealt{horn:1972,gazdar:1979}). In this way, ``Chris owns some
of Johnny Cash's albums'' is strengthened to convey that she owns {\em some but not all}
albums. According to the majority view, this is a pragmatic inference, not part of the
conventional meaning.

In the following we consider a specific application of the model from Section~\ref{sec:model}
which allows us to address the question if or when scalar inferences might (not) lexicalize. We
consider what is perhaps one of the simplest non-trivial setups that speak to this matter and
reflect on its limitations in Section~\ref{sec:discussion}. The setup is introduced in
Section~\ref{sec:setup:-stat-conc}. Section~\ref{sec:simulation-results} describes simulations
and their results.

% Our initial question about the division of labor between semantics and pragmatics can be narrowed to the case of scalar implicatures by asking for a justification for the lack of lexical upper-bounds in weak scalar alternatives. That is, we ask why lexical meanings that lack upper-bounds and convey it pragmatically are regularly selected for over alternatives such as that of codifying the bound semantically. More poignantly, would it not serve language users better if weak(er) expressions such as {\em warm}, {\em or}, {\em some} and {\em big} were truth-conditionally incompatible with stronger alternatives such as {\em hot}, {\em and}, {\em all} and {\em huge} to avoid misunderstanding?  This question is particularly striking considering the number of expressions that license such inferences across natural languages. 


\subsection{Setup}
\label{sec:setup:-stat-conc}

To fill the model from Section~\ref{sec:model} with life, we need to specify the sets of
states, messages and lexica we consider (Section~\ref{sec:stat-mess-conc}). Additionally, we want to
explore the effects of a learning bias in favor of simple lexical representations. One way of
motivating and formalizing such a bias is introduced in Section~\ref{sec:an-induct-learn}.


\subsubsection{States, messages, lexical representations, and lexica} 
\label{sec:stat-mess-conc}

Consider a state space with three states $\States = \set{\snone, \ssome, \sall}$ and think of
it as a partition of possible worlds into cells where none, some or all of the $A$s are $B$s,
for some arbitrary fixed predicates $A$ and $B$. Eight lexical representations can be distinguished based on
their truth or falsity in three world states, six of which are not contradictory or
tautological (see Table~\ref{tab:concepts} below).

A lexicon $L$ is a mapping $\Messgs \rightarrow R$ from messages to representations. With three
messages there are $6^3 = 216$ possible lexica. Some assign the same representations to more
than one message and others lexicalize the same representations but associate them with
different messages. Out of these possible lexica, three kinds are of particular
relevance. First, lexica that assign the same lexical representations to more than one
message. Such lexica lack in expressivity but may be favored by particular learning biases
nonetheless (see below). Second, lexica that conventionalize upper-bounds to realize a
one-to-one mapping of messages to states. Finally, lexica that do not lexicalize an upper
bound but allow it to be conveyed pragmatically due to the presence of a stronger lexical
item. There are six lexica of the second kind and six of the third.  The following three lexica
exemplify each kind:
\begin{align*}
  \begin{blockarray}{lccc}
    & & \underline{\Lall} & \\
    & \mygray{\mnone} & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[ccc]}
     \mygray{\snone}  & 0 & 0 & 0\\
     \mygray{\ssome}  & 0 & 0 & 0\\
    \mygray{\sall}   & 1 & 1 & 1\\
    \end{block}
% \\ & & P(\Lall)\rlap{ $\propto 9$} & \\
  \end{blockarray} & &
 \begin{blockarray}{lccc}
    & & \underline{\Lbound} & \\
    & \mygray{\mnone} & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[ccc]}
       & 1 & 0 & 0\\
       & 0 & 1 & 0\\
       & 0 & 0 & 1\\
    \end{block}
%  \\& & P(\Lbound)\rlap{ $\propto 96$} & \\
  \end{blockarray} & &
  \begin{blockarray}{lccc}
    & & \underline{\Llack} & \\
    & \mygray{\mnone} & \mygray{\msome} & \mygray{\mall} \\
    \begin{block}{l[ccc]}
       & 1 & 0 & 0\\
       & 0 & 1 & 0\\
       & 0 & 1 & 1\\
    \end{block}
%   \\& & P(\Llack)\rlap{ $\propto 48$} & \\
  \end{blockarray}
\end{align*}
Lexicon $\Lall$ is clearly bad for communication: all message and interpretation choices will
be equally likely for all types; no information about the observed world state will be conveyed
by its users. In contrast, users of $\Lbound$ can communicate world states perfectly, no matter
whether they are literal or pragmatic users. Users of $\Llack$ can also communicate information
about the actual world state but need pragmatic language use to approximate a one-to-one
mapping between message use and states (see Section~\ref{sec:languages+use}).  

Recall that types are a combination of a lexicon and a manner of language use. We analyze the
model's predictions in populations of types with one of the two behaviors introduced earlier;
literal or pragmatic. The former correspond to level $0$ reasoners and the latter to level
$1$. Accordingly, we consider a total of $432$ types. Six are variants of pragmatic language
users with $\Llack$-like lexica. We refer to these as \emph{target types}, because they
represent lexica and language use that conform to the majority view of scalar
implicatures. Twelve types are either literal or pragmatic speakers with lexica of the
$\Lbound$ kind. We refer to these as \emph{competitor types}, because they are expected to be
the target types' main contenders in evolutionary competition. Finally, note that while
different types may lexicalize the same representations, they may nevertheless map different
states to different overt messages. More informally, they speak different languages that
lexicalize the same concepts. Consequently, more often than not different variants of the same
kind fail to understand each other.




\subsubsection{An inductive learning bias for semantic simplicity}
\label{sec:an-induct-learn}

There is a growing effort to develop empirically testable representational languages that allow
for the measure of semantic complexity. For instance, so-called {\em languages of thought}
(LOTs) have been put to test in various rational probabilistic models that show encouraging
results (see, e.g., \citealt{katz+etal:2008}, \citeauthor{piantadosi+etal:underreview} under review; 2012, and \citealt{piantadosi+jacobs:2016} for recent discussion). At its
core, a LOT defines a set of operations and composition rules from which lexical representations can be
derived. As a first approximation and for the sake of concreteness, we follow this approach to
motivate and formalize a preference of learners for simpler semantic representations
(\citealt{feldman:2000, chater+vitanyi:2003, piantadosi+etal:2012a,kirby+etal:2015}, \citeauthor{piantadosi+etal:underreview} under review). 
In a weighed generative LOT a representation's complexity is a function of its derivation cost.

\begin{table}
  \centering
  \begin{align*}
    R & \rightarrow_2 R \wedge R 
    & R & \rightarrow_2 \neg R  \\ 
    R & \rightarrow_1 X \subseteq X &
    R & \rightarrow_1 X \neq \emptyset &
    R & \rightarrow_1 X = \emptyset  \\
    X & \rightarrow_1 \set{A,B} & 
    X & \rightarrow_1 X \cap X & 
    X & \rightarrow_1 X \cup X    
  \end{align*}
  \caption{Toy grammar in a set-theoretic LOT with weighted rules.}
  \label{tab:grammar}
\end{table}

Our toy grammar of lexical representations is given in Table~\ref{tab:grammar}. This grammar uses basic
set-theoretic operations to form expressions which can be evaluated as true or false in states
$\snone$, $\ssome$, or $\sall$ from above. Applications of generative rules have a cost attached
to them. Here we simply assume that the formation of Boolean combinations of representations incurs 2
cost units, while all other rule applications incur only 1 cost unit. Table~\ref{tab:concepts}
lists all six lexical representations relevant here, their truth conditions and the simplest formula that
expresses this representation in the grammar in Table~\ref{tab:grammar}.

\begin{table}
  \centering
\begin{center}
  \begin{tabular}{lccclc}
    \toprule
    intuitive name
    & \snone
    & \ssome
    & \sall
    & least complex formula
    & complexity
    \\ \midrule
    ``all''
    & 0
    & 0
    & 1
    & $A \subseteq B$
    & $3$
    \\
    ``some but not all''
    & 0
    & 1
    & 0
    & $A \cap B \neq \emptyset \wedge A \neq \emptyset$
    & $8$
    \\    
    ``some''
    & 0
    & 1
    & 1
    & $A \cap B \neq \emptyset$
    & $4$
    \\
    ``none''
    & 1
    & 0
    & 0
    & $A \cap B = \emptyset$
    & $4$
    \\
    ``none or all''
    & 1
    & 0
    & 1
    & $\neg(A \cap B \neq \emptyset \wedge A \neq \emptyset)$
    & $10$
    \\
    ``not all''
    & 1
    & 1
    & 0
    & $\neg (A \subseteq B)$
    & $5$
    \\
    \bottomrule
  \end{tabular}
\end{center}
\caption{Available lexical representations and their minimal derivation cost.}
\label{tab:concepts}
\end{table}

A complexity measure for lexical representations from Table~\ref{tab:concepts} is used to define a
learning bias that favors simpler representations over more complex ones. The prior probability of a
type is just the prior probability of its lexicon. The prior of a lexicon is a function of the
complexity of the lexical representations in its image set. Lexica with simpler representations accordingly have a
higher prior. One simple way of defining such priors over lexica (and thereby types) is:
\begin{align*}
  P(L)  & \propto \prod_{r \in Im(L)} P(r)   \ \text{, with} & 
  P(r) & \propto \max_{r'}Compl(r') - Compl(r) + 1\,,
\end{align*}
where $Compl(r)$ is the complexity of the minimal derivation cost of representation $r$ according to the
LOT-grammar (see Table~\ref{tab:concepts}). Applied to our space of lexica, this construal assigns the highest probability 
to a lexicon of type $\Lall$, which only uses the simplest lexical representation ``all'' for
all messages. Lexica of type $\Llack$ are less likely, but more likely than $\Lbound$. 

There are many ways to define priors over lexica (see, e.g., \citealt{goodman+etal:2008,
  piantadosi+etal:2012a}) but the key assumption here, common to all of them, is that simple
representational expressions should be favored over more complex ones. We should stress that
these details -- from the generative grammar to its complexity measure -- are to be regarded as
one convenient operationalization of one general approach to explicating learning biases; this
is not a commitment that this general approach is necessarily superior or that, within it, this
particular instrumentalization is the single most plausible.


\subsection{Simulation results}
\label{sec:simulation-results}

Recall that there are three parameters: soft-max parameter $\lambda$ affects how strongly
speakers favor messages that appear best from their subjective point of view; the
bottleneck size $k$ influences how faithfully learners can identify their teacher type; $l$
defines the learners' disposition towards choosing the most likely teacher type from the
posterior distribution. We expect that competitor types (types with lexica of the kind
$\Lbound$) have a fitness advantage over target types (pragmatic agents with lexica of the kind
$\Llack$), especially for very low levels of $\lambda$. Selection based on fitness alone may
therefore not lead to prevalence of target types in the population. On the other hand, lexica
of type $\Llack$ are simpler than those of type $\Lbound$ by the postulated measure from
above. This may make them more likely to be adopted by learners, especially when $k$ is low so
that different teacher types are relatively indistinguishable based on their behavior and when
$l$ is high. Still, lexica of the kind $\Lall$ are in turn even more likely \emph{a priori} than
lexica of the kind $\Llack$. Simulation results will shed light on the question whether target
types can emerge, and for which parameter constellations.

To better understand the workings of a pressure towards communicative efficiency and a pressure
towards learnability, we look at the behavior of the replicator and mutator step first in
isolation, and then in combination. All simulation runs are initialized with an arbitrary
distribution over types, constituting a population's first generation. All reported results are
the outcome of 50 update steps. These outcomes correspond to developmental plateaus in which
change is, if not absent, then at least very slow. In other words, even if the resulting states
do not correspond to an eventual attracting state, they characterize almost stationary states
in which the system remains for a very long time. As specified in \S\ref{sec:learnability}, the
mutation matrix $Q$ can be obtained by considering all possible state-message sequences of
length $k$. Given that this is intractable for large $k$, the sets of data which learners are
exposed to are approximated by sampling $250$ $k$-length sequences from each type's production
probabilities.


\subsubsection{Replication only: selection based on communicative success} 


\begin{figure}[t]
\centering
\includegraphics[width=\textwidth,height=8cm, keepaspectratio]{./plots/alt-fig1-onlyr}
\caption{Proportion of target types, pragmatic competitor types, pragmatic $\Lall$, and the population's majority type, in representative populations after $50$ generations under only a
  pressure for communicative success.}
\label{fig:only-R}
\end{figure}

Selection based on communicative success is sensitive to $\lambda$ since $\lambda$
influences signaling behavior, which in turn determines communicative success. This is
showcased in Fig. \ref{fig:only-R}, which shows the proportion of target types, pragmatic competitor types and pragmatic $\Lall$ in a representative population after $50$ replicator steps. The plot also indicates the proportion of the
\emph{majority type}, i.e., the type with the highest proportion in the final population. With
low $\lambda$ many types have very similar behavior, so that evolutionary selection lacks grip
and becomes very slow. The result is a very long transition with near stagnancy in a rather
homogeneous population with many types. Conversely, higher $\lambda$ promotes 
less-stochastic pragmatic behavior, widening the gap in expressivity between types and
promoting more homogeneous populations. As suggested by Fig. \ref{fig:only-R}, the majority
in most populations is not one of the six pragmatic $\Llack$-style types. That is, a pressure
only for communicative success does not lead to a prevalence of target types under any
$\lambda$-value. For instance, with $\lambda = 20$ $1000$ independent populations only had $11$
cases in which the target type was the majority type, corresponding to a mean proportion of
$.003$ across populations. By contrast, in $913$ cases the majority types had $\Lbound$ with
close to an even share between literal ($454$) and pragmatic types ($459$), corresponding to a
mean proportion of about $.48$ taken together.


\subsubsection{Iterated learning only}

\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth,height=8cm,keepaspectratio]{./plots/alt-fig2-onlym-pr}
\caption{Proportion of target types, pragmatic competitor types, pragmatic $\Lall$, and the population's majority type in representative populations after $50$ generations under only a
  pressure for learnability ($\lambda = 20, k = 5$). The learning prior is shown in the
  right-most plot with top-most groupings corresponding to types, literal and pragmatic, with
  lexica of kinds $\Llack$, $\Lbound$, and $\Lall$.}
\label{fig:only-M}
\end{figure}

The effect of iterated learning without a pressure for communicative success using either posterior
sampling ($l = 1$) or a stronger tendency towards posterior maximization ($l = 15$) is shown in
Fig.\ref{fig:only-M} together with the prior over types. The prior shows that while users of
$\Llack$ are not the most favored by the inductive bias (compared, e.g., to $\Lall$) they are
nevertheless more advantaged than others, such as $\Lbound$, in virtue of the relatively simple
semantics they conventionalize (see Section~\ref{sec:an-induct-learn}). Crucially, $\Llack$
enables its users to convey each state with a single message when combined with pragmatic
reasoning and sufficiently high $\lambda$. This makes it less likely to be confused with
other types if the learning data is not too sparse ($k \geq 5$). Put differently, learners have
a higher propensity to infer pragmatic $\Llack$ when the teacher's type produces very similar
data, such as when using $\Lbound$. Moreover, $\Llack$ is less likely to be confused with
types with different observable behavior because its pragmatic use approximates a one-to-one
form-meaning mapping. As a consequence, a stronger propensity to maximize the posterior
increases their proportion in the population.

However, in contrast to a pressure only for communicative success with high $\lambda$ (see
Fig.~\ref{fig:only-R}), learnability alone does not succeed in selecting for a single
prevalent type. Indeed, all six target types tend to coexist at roughly equal proportion. Each
is passed on to the next generation with the same faithfulness and, differently from a pressure
for communicative success, they do not stand in competition with each other. In $1000$
independent populations with $\lambda = 20$ all majority types were target types,
with each reaching approximately the same proportion of users in the population. As
with a pressure only for communicative success, low values of $\lambda$ make the differences in observable behavior
across types less pronounced and therefore reflect the learners' inductive bias more
faithfully, favoring functionally deficient but a priori preferred types such as those that use
$\Lall$. A pressure for learnability alone may consequently lead to a spread of communicatively
suboptimal types that are easier to learn. In the extreme, when $l = 1$ and $\lambda = 1$ all
of $1000$ independent populations had users of $\Lall$ as majority types.

\subsubsection{Combining pressures of communicative success and learnability}

Pressure for communicative success and learnability are not sufficient on their own to have a
single target type dominate the population. When pressured for communicative success, the slight
communicative advantage of $\Lbound$ users leads to their prevalence. When pressured for
learnability, pragmatic $\Llack$ is promoted over functionally similar but semantically more
complex alternatives such as $\Lbound$. However, learnability alone does not foment the
propagation of a single target type across the population.

\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth,height=8cm,keepaspectratio]{./plots/alt-fig3-r+m}
\caption{Proportion of target types, pragmatic competitor types, pragmatic $\Lall$, and the population's majority type in representative populations after $50$ generations under both pressures ($k = 5$).}
\label{fig:rmd}
\end{figure}

Fig. \ref{fig:rmd} illustrates the combined effects of both pressures for a sample of
$\lambda$ and $l$ values. These results show that an inductive learning bias for simpler
semantics in tandem with functional pressure can lead to the selection of a single target
type, and so to a division of labor between semantics and pragmatics.
The proportion of a single majority target type increases with $\lambda$ and
$l$. Pressure for communicative success magnifies the effects of iterated learning and dampens
the proliferation of multiple types of a kind that are equal in expressivity {\em and} learnability. A
pressure towards learnability favors the transmission of simpler semantics and thereby selects
for pragmatic language use.

As before, low $\lambda$ and $l$ lead to the prevalence of communicatively suboptimal types
that are a priori favored, such as $\Lall$. An increase in $\lambda$ leads to the selection of
target types but does not lead to monomorphic populations if learners sample from the
posterior. Finally, a combination of high $\lambda$ and $l$ leads to increasing proportions of
a single majority target type. This joint influence is summarized in Fig. \ref{fig:diff}, which shows the mean difference between the highest target type and the highest proportion of a type of a different kind in $1000$ independent populations across $\lambda$ and $l$ values. Higher values of $\lambda$ and $l$ increase the prevalence of a single target type, whereas lower values lead to less pronounced differences, with a valley resulting from low $\lambda$ and high $l$ (cf. Fig. \ref{fig:rmd} with $\lambda = 1$ and $l=15$). 

\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth,height=10cm,keepaspectratio]{./plots/fig4-3d-incumbents-difference} %alternatively without '-3d'
\caption{Mean difference between proportion of highest target type and highest other type in
  $1000$ independent populations after $50$ generations under both pressures ($k = 5$).}
\label{fig:diff}
\end{figure}

Effects of manipulating the sequence length $k$ have not been addressed so far, but are rather
predictable: small values lead to more heterogeneous populations that reflect the learner's
prior more faithfully. This is due to the fact that the likelihood that a small sequence was
produced by any type is relatively uniform (modulo prior). By contrast, larger values
increasingly allow learners to differentiate types with different signaling behaviors.

To recapitulate, other than the involvement of pressure on both communicative success and learnability,
the resulting proportion of pragmatic $\Llack$ speakers primarily hinges on three
factors. First, the degree, captured by $\lambda$, to which agents try to maximize
communicative success from their own subjective point of view. Second, the inductive bias,
which leads learners to prefer simpler over more complex semantic representations in
acquisition. Lastly, the learning behavior, captured by parameter $l$, where approximating a
MAP strategy magnifies the effects of the learning bias in tandem with replication. More broadly,
target types, which represent the majority view of scalar implicatures, can come to
dominate the population if three assumptions are met: (i) language is pressured toward both
communicative success and learnability; (ii) pragmatic language use is an option; (iii) learners prefer
simpler over more complex lexical representations and exhibit a tendency towards the
acquisition of the type that best explains the learning data.


\section{General discussion}\label{sec:discussion}

The approach introduced here combines game theoretic models of functional pressure towards
successful communication \citep{nowak+krakauer:1999}, effects of transmission perturbations on
(iterated) language learning \citep{griffiths+kalish:2007}, probabilistic speaker and listener
types of varied degrees of pragmatic sophistication \citep{frank+goodman:2012,
  franke+jaeger:2014} as well as reasoning about unobservable lexical representations
\citep{bergen+etal:2012,bergen+etal:2016}. This allows for a conceptual investigation of the
co-evolution of conventional meaning and pragmatic language use. Main contributions of the
model are (i) its modular separation of communicative success and learnability on evolutionary
trajectories, (ii) the characterization of language learning as a joint inference over
pragmatic behavior and lexical meaning, and (iii) the possibility to trace the co-evolution of
conventional semantics and pragmatic use.

With respect to (i), \citet{kirby+etal:2015} propose a comparable model of the interaction
between a lexicon's expressivity and its learnability. A main difference is that here we
considered a pressure for mutual understanding within a generation that may indirectly select
for expressive types, i.e., those that can convey states unequivocally, whereas Kirby et
al. only consider the bearing that the latter ability has on the production of learnable
data. We see two main reasons for considering communicative success rather than just
expressivity, and over only considering it in learning. First, learnability alone can lead to the
acquisition of functionally defective languages, as showcased by $\Lall$
(cf. \citealt{kirby+etal:2008,silvey+etal:2014}, and see \citealt{fay+etal:2013} for review of
laboratory results). Second and more importantly, types may be equally expressive but their
performance as means of information transfer crucially depends not only on themselves but on
the population they find themselves in (cf. the competition of target types in Fig. 
\ref{fig:rmd} and their lack of competition in Fig. \ref{fig:only-M}). That is, we contend
that having an expressive lexicon that generates learnable data does not in itself capture a
type's arguably central communicative function of transferring information to peers. Taking communication
into consideration in turn allows the model to be responsive to the task for which language is learned. 

The main result of our case study is that types that correspond to the majority view of scalar
implicatures (scalar readings are non-lexicalized pragmatic enrichments) can come to dominate a
population. This can happen under the assumption that simpler semantic representations are more
likely to be learned (cf. \citealt{chater+vitanyi:2003}). Pragmatic language use can be
recruited indirectly by a preference for simpler lexical representations. Under this view, semantics
and pragmatics play a synergic role: pragmatic use allows maintenance of simpler representations;
pressure towards representational simplicity indirectly promotes pragmatic over literal
language use. As a consequence, iterated transmission and use of language lead to a
regularization that may explain the lack of lexicalization of systematic pragmatic enrichments.

While the results of this case study are interesting, they also raise a number of critical
issues. First of all, while many favorable parameter settings exist which lead to a prevalence
of target types, other types are usually represented in non-negligible proportions. This may
just be a technical quirk of the mutator step. But there is a related issue of empirical
importance. Several experimental studies on scalar implicatures suggest that participants can
be classified as either semantic or pragmatic users of, in particular, \emph{some}
\citep[e.g.,][]{BottNoveck2004:Some-Utterances,NieuwlandDitman2010:On-the-incremen,DegenTanenhaus2012:Processing-Scal}. The
former consistently accept \emph{some} where \emph{all} would be true as well, the latter do
not. Interestingly, in our simulations when a target type is the majority type an inflated
proportion of the population uses compatible lexica with a lexicalized upper
bound. Particularly in those parameter settings where the prior influences the outcome less. In
other words, we do find a tendency toward a similar co-existence of semantic and pragmatic
types. Whether this analogy has any further explanatory value is an interesting path for future
exploration.

Another important issue that is not addressed in the model are potential costs associated with
pragmatic reasoning. Here, we simply assumed that literal and pragmatic reasoning strategies
exist from the start and are equally costly to apply. In contrast, empirical results suggest
that the computation of a scalar implicature may involve additional cognitive effort
\citep[e.g.,][]{BrehenyKatsos2006:Are-Generalised,deNeys+schaeken:2007,huang+snedeker:2009,Jr.Bailey2013:Possibly-all-of}. Extensions
of the model presented here to include processing costs for pragmatic language use would be
interesting future work. It seems plausible that effects of reasoning cost may trade off with
the frequency with which a given scalar expression is used. It may be that frequently drawn
scalar implicatures lexicalize to avoid cost, whereas infrequent ones are derived on-line to
avoid more complex lexical representations during acquisition. Such a prediction would lend itself to
empirical testing in line with a recent interest in differences between various scalar
implicature triggers \citep{Tielvan-TielMiltenburgvan-Miltenburg2014:Scalar-Diversit}. 

Our case study could be criticized as follows: all it shows is that scalar implicatures do not
lexicalize because upper bounds are dispreferred lexical representations. This criticism would be too
superficial and highly unjust. Dispreferred lexical representations can thrive under evolutionary
selection. Lexicalized upper-bounds can dominate a population because they may boost
communicative success. But they do not have to. Moreover, even without selective pressure
for communicative success, it is not necessarily the case that the types that are most
likely \emph{a priori} will dominate. The dynamics of iterated learning are not that
trivial. Iterated learning does not necessarily promote the \emph{a priori} more likely type,
but tends to promote a type $t$ based on a gradient of how many other types might likely mutate
into $t$, so to speak. Taken together, without an explicit model of the interaction between
pressure for communicative success and learnability, it is far from trivial to judge whether or when
preferred or dispreferred representations can be adaptive. This is why a major contribution of this
paper is the arrangement of many different ingredients into a joined model of the co-evolution
of lexical meaning and pragmatic use. 

What is more, it is not that we just assumed a prior disadvantage of lexicalized upper
bounds. We tried to motivate and formalize a general assumption about lexical representations'
complexity with a concrete, albeit provisional proposal. The specification of a learning bias
in terms of a ``grammar of representations'' can and should be seen critically, however. Much
depends on the primitives of such a grammar. For instance, the lexical representation ``none or
all'' is the most complex in Table~\ref{tab:concepts}. But consider adding a new primitive
relation between sets $A \smile B$ which is true if and only if
$\neg(A \cap B \neq \emptyset \wedge A \neq \emptyset)$. The lexical representation ``none or
all'' would then be one of the simplest. Clearly, further research, empirical and conceptual,
into the role of representational complexity, processing costs and learning biases is
needed. The model here makes a clear and important contribution nonetheless: it demonstrates
how simplicity of representations can interact with use and evolutionary selection and shows
that for simple representations to emerge it may require pragmatic strategies to compensate
their potential expressive deficiencies. Hence a model of co-evolving semantics and pragmatics is
needed. Future work should also include the possibility that representational simplicity may
itself be a notion that is subject to evolutionary pressure
\citep[cf.][]{ThompsonKirby2016:Culture-Shapes-}, as well for the evolution of elements that define
the agents' cognitive make-up; $\lambda$ and $l$.

Finally, our case study should not be interpreted as a proposal for a definite explanation of
how scalar implicatures evolved. Other factors should be considered eventually even if they
will lead to much more complex modeling. One such factor is the observation that
non-lexicalized upper bounds allow a broader range of applicability, e.g., when the speaker is
not certain as to whether \emph{all} is true. This may suggest an alternative and purely
functionalist argument for why upper-bounded meanings do not conventionalize: should contextual
cues provide enough information to the hearer to identify whether a bound is intended to be
conveyed pragmatically, then this is preferred over expressing it overtly through longer
expressions, e.g., by saying {\em some but not all} explicitly. Importantly, although
morphosyntactic disambiguation may be dispreferred due to its relative length and complexity
\citep{piantadosi+etal:2012b}, it allows speakers to enforce an upper-bound and override
contextual cues that might otherwise mislead the hearer. In a nutshell, this explanation posits
that scalar implicatures fail to lexicalize because, all else being equal, speakers prefer to
communicate as economically as possible and pragmatic reasoning enables them to do so. What
this alternative argument does not explain is why functional pressure does not lead to the
emergence of different, equally costly lexical items to express different knowledge states of
the speaker (cf. \citealt[252-267]{horn:1984},
\citealt{horn:1972,traugott:2004,vdAuwera:2010}). For instance, to the emergence of two expressions for each weak
scalar expression; one with and one lacking an upper-bound.  Looking at pressure from
learnability might come in again. Beyond scalar implicatures, the model can generate predictions about
likely lexicalization trajectories of pragmatic inferences, or a lack thereof. In this realm
an interesting issue is whether proposed principles, such as the semantic conventionalization of once highly context-dependent inferences if they become regular enough \citep{levinson:2000,traugott:2004}, can be given
a formal rationale and inform postulated directionalities of change. The present work made a first start and gave a framework for
exploring exactly these issues systematically.


\section{Conclusion}
The cultural evolution of meaning is influenced by intertwined pressures. We set out to
investigate this process by putting forward a model that combines a pressure toward successful
information transfer with perturbations that may arise in the transmission of linguistic
knowledge in acquisition. Its objects of selection and replication are pairs of lexical
meanings and patterns of language use. This allows the model to trace the interaction between
conventional meaning and pragmatic use. Additionally, it takes the challenge seriously of
neither semantics nor pragmatics being directly observable. Instead, learners need to infer
these unobservables from overt data that results from their combination.  These components and
their mutual influence were highlighted in a case study on the lack of lexical upper-bounds in
weak scalar expressions that showed that, when pressured for learnability and communicative
success, the former force drives for simpler semantic representations inasmuch as pragmatics
can compensate for lack of expressivity in use. That is, the relative learning advantage of
simpler semantics in combination with functional pressure in use may offer an answer to why
natural languages fail to lexicalize systematic pragmatic inferences. And, more broadly, lead to a
division of labor between semantics and pragmatics.

\bibliographystyle{chicago}
\bibliography{./bounds-rmd}




\end{document}
