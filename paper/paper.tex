\documentclass[a4paper]{article}

\usepackage{geometry}
\usepackage{natbib}
\bibpunct[:]{(}{)}{,}{a}{}{;}

%--------------------
%\usepackage{gb4e}
%\noautomath

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{nicefrac}
%\usepackage{stmaryrd}
%\usepackage{multicol}
%\usepackage{graphicx}
\usepackage{color}
%\newcommand{\mvalueof}[1]{\llbracket#1\rrbracket}
\newcommand{\citeposs}[2][]{\citeauthor{#2}'s (\citeyear[#1]{#2})}
\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}} 

\newcommand{\hl}[1]{\textcolor[rgb]{.8,.33,.0}{#1}}% prints in orange
%\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
%\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
%\newcommand{\sbna}{\exists\lnot\forall}

%--------------------
%
%\usepackage{setspace}
%\onehalfspacing
%
%-------------------


\title{Communicative pressures at the semantics-pragmatics interface:\\ Learning biases may prevent the lexicalization of pragmatic inferences}

\author{%\bf NAME1 and NAME2\\
    ( -- draft \today --- )
}


\date{}

\begin{document}
\maketitle

\begin{abstract} Certain semantic structures enable for pragmatic enrichments in a notably productive fashion. This raises the challenge to justify their regular selection over alternatives that codify semantically what is conveyed pragmatically. This issue is particularly puzzling under a purely functional perspective. To address this challenge, we propose a general model for the analysis of linguistic pressures that integrates iterated Bayesian learning in the replicator-mutator dynamics. This model allows for population-level analyses of the effects of such pressures on probabilistic language users with varied degrees of pragmatic sophistication and distinct languages. We showcase the model's predictions in a case study on the (lack of) lexicalization of scalar implicatures. The results suggest simpler semantic representations to be selected for when languages are pressured towards learnability, provided that pragmatic reasoning can compensate for the disadvantage in expressivity that users of such languages otherwise incur.


\section{The semantics-pragmatics divide}\label{sec:introduction}

\end{abstract}

In linguistic theorizing, it is common to draw a distinction between semantics and pragmatics. Broadly speaking, the former concerns the truth-conditional content of expressions, whereas the latter concerns information beyond literal meanings and their composition. Under this view, the information conveyed by an utterance is seldom, if ever, solely determined by semantics, but rather in tandem with pragmatics. 

Much research at the semantics-pragmatics interface has been aimed at characterizing expressions in terms of either domain, or their interplay. As a consequence, their distinction has played an important role in the field's theoretical and experimental development. However, an issue that has received little attention is the justification of semantic structure in light of pragmatics. That is, the selection and pervasiveness of particular semantics under consideration of the regular informational enrichment provided by pragmatics. 

Similar questions have lead to a recent surge of models that aim to analyze the development and selection of linguistic features (see \citealt{steels:2015} and \citealt{tamariz+kirby:2016} for recent overviews). Our starting point is given by the overarching argument that has crystalized from accumulated mathematical, experimental and cross-linguistic evidence in this literature: Natural languages need to be well-adapted to communicative needs within a linguistic community, but also need to be learnable to survive their faithful transmission across generations. More succinctly; natural languages are pressured for expressiveness as well as learnability.  

%While their efforts have largely concentrated on compositional and combinatorial systems, 
We proceed by modeling these pressures using the replicator-mutator dynamics (see \citealt{hofbauer+sigmund:2003} for an overview). This allows us to inspect their dynamics by combining functional pressure on successful communication, effects of learning biases on (iterated) Bayesian learning \citep{griffiths+kalish:2007}, and  probabilistic models of language use in populations with distinct lexica \citep{frank+goodman:2012,franke+jaeger:2014, bergen+etal:2016}. 

%In this way, the model establishes a link between models of synchronic probabilistic rational language use and diachronic models of cultural evolution. 

%We then analyze the prevalence of a lack of semantic upper-bounds in the literal meaning of weak scalar expressions. We show that a lack of upper-bounds is selected for when learners are biased towards simpler semantic representations, provided they have means to convey upper-bounds, i.e., provided that they can be derived via pragmatic reasoning.  


\section{Simplicity, expressivity, and learnability}

The emergence and change of linguistic structure is influenced by many factors, ranging from biological and socio-ecological to cultural \citep{steels:2011,tamariz+kirby:2016}. Social and ecological pressures determine communicative needs, while biology determines the architecture that enables and constrains their means of fulfillment. In the following, our focus lies on the latter, cultural factor, wherein certain processes of linguistic change are understood as shaped by use and transmission. That is, as a result of cultural evolution.

At latest since \citeposs{zipf:1949} rationalization of the observation that word frequency rankings can be approximated by a power law distribution as competing hearer and speaker preferences, the idea that linguistic selection and change are driven by communicative pressures has played a pivotal role in synchronic and diachronic analyses (e.g. \citealt{martinet:1962, horn:1984,jaeger+vRooij:2007,jaeger:2007, piantadosi:2014,kirby+etal:2015}). As noted above, expressivity and learnability are two key instances of such competing pressures. Their opposition becomes particularly clear when considering their consequences in the extreme (cf. \citealt{kemp+regier:2012,kirby+etal:2015}). On the one side, a language with a single form is easy to learn but lacking in expressivity for most applications. On the other, a language that associates a distinct form with all possible meanings its users may want to convey is maximally expressive but challenging to acquire.

The most prominent problem that arises from the tension between learnability and expressivity is that of acquiring a language to express a potentially infinite set of meanings through finite means \citep{kirby:2002}. However, this so-called transmission bottleneck is not the only challenge learners confront. More important for our purposes is the problem of selecting particular hypotheses out of a potentially infinite space of alternatives compatible with the data learners are exposed to. At the semantics-pragmatics interface this concerns the selection between functionally similar, if not identical, lexical meanings.  In the following, we assume an integeral part of the answer to be that learners are a priori biased towards simpler, more compressed, representations. This corresponds to the argument that rational learners should prefer simpler over more complex explanations of data \citep{feldman:2000, chater+vitanyi:2003, piantadosi+etal:2012a, kirby+etal:2015,piantadosi+etal:underreview}. In linguistics, a drive for simplicity has been argued to underpin speaker preferences for brevity and ease of articulation, as well as to pressure languages towards lexical ambiguity and grammatical compression \citep{zipf:1949,grice:1975,piantadosi+etal:2012, kirby+etal:2015}. \hl{As a broader cognitive principle, the use of simplicity as means to select betweeen hypotheses that fit the data has a long standing tradition. Crucially, \citet{chater+vitanyi:2003} provide strong arguments for a prior towards simpler explanations. {\bf this needs to be expanded}}

The remainder of this section introduces the individual components of the model in more technical detail. That is: (i) languages and their use, (ii) pressures towards expressivity and learnability, regulated by the replicator and mutator dynamics, respectively, as well as (iii) a bias towards simpler semantic representations, codified as a language learner's prior. After laying out the model, we discuss its application to the lack of lexicalization of scalar implicatures.



%The observation that monomorphemic expression that lexically rule out stronger alternatives (e.g. {not all} as hypothetical `nall') are unattested across languages has received substantial argumentative support (most prominently in \citealt[252-267]{horn:1984} but also e.g. in \citealt{horn:1972,traugott:2004,vdAuwera:2010}). To the best of our knowledge, this claim stands unchallenged.

\subsection{Languages and linguistic behavior}
Lexica are taken to codify the truth-conditions of a language's expressions, i.e., its semantics. Given a state of affairs and a lexicon, language users can judge whether an expression is true or false. A convenient way to represent such lexica is by $(|S|,|M|)$-Boolean matrices, where $S$ is a set of states of affairs and $M$ the set of messages in the lexicon \citep{franke+jaeger:2014}. For instance, the following two lexica fragments determine the truth-conditions of two messages, $m_1$ and $m_2$, for two states, $s_1$ and $s_2$:

\begin{centering}
$L_a$ = \bordermatrix{~ & m_1 & m_2 \cr 
                  s_1 & 1 & 0 \cr
                  s_2 & 1 & 1 \cr} \hspace{2cm} $L_b$ = \bordermatrix{~ & m_1 & m_2 \cr 
                  s_1 & 1 & 0 \cr
                  s_2 & 0 & 1 \cr}\\[0.5cm]
\end{centering}


In words, according to lexicon $L_a$, $m_1$ is true of $s_1$ as well as $s_2$. In contrast, message $m_1$ is only true of $s_1$ in $L_b$. Otherwise, the two languages are truth-conditionally equivalent. 

To make the distinction betweeen semantics and pragmatics precise, we distinguish between two kinds of linguistic behavior. {\em Literal interlocutors} produce and interpret messages literally. That is, their linguistic choices are solely guided by their lexica. In contrast, {\em pragmatic interlocutors} engage in mutual reasoning to inform their choices. For instance, given that $m_2$ is unambiguously associated with state $s_1$, a rational speaker of $L_a$ who reasons about her addressee should use $m_1$ to signal state $s_2$. Analogously, should rational hearers expect their interlocutors to reason along these lines, they will interpret the messages accordingly. Note in particular that according to this sketch of the pragmatic strenghtening of $m_1$, $L_a$ is indistinguishable from $L_b$ in terms of expressiveness.

Following models of rational language use such as Rational Speech Act models \citep{frank+goodman:2012} and their game-theoretic counterparts \citep{benz+etal:2005a,franke:2009,franke+jaeger:2014}, this kind of signaling behavior is captured by a hierarchy over reasoning types. The hierarchy's bottom, level $0$, corresponds to literal language use. Language users of level $n + 1$ behave rationally according to (expected) level $n$ behavior of their interlocutors. The behavior of literal level $0$ and pragmatic level $n+1$ hearers of a language $L$ is is captured by their respective selection functions in (\ref{h:level0}) and (\ref{h:leveln}). Mutatis mutandis for the speaker functions in (\ref{s:level0}) and (\ref{s:leveln}).

\begin{flalign}
&H_{0}(s|m;L) \propto pr(s) L_{sm} \label{h:level0}\\
&S_{0}(m|s;L) \propto \exp(\lambda \; L_{sm}) \label{s:level0}\\
&H_{n+1}(s|m;L) \propto pr(s) S_{n}(m|s;L) \label{h:leveln}\\
&S_{n+1}(m|s;L) \propto  \exp(\lambda \; H_{n}(s|m;L)^\alpha) \label{s:leveln}
\end{flalign}

According to (\ref{h:level0}), a literal hearer's interpretation of a message $m$ as a state $s$ depends on her lexicon, weighted by her prior over states, $pr \in \Delta(S)$. The latter plays an important role when hearers face ambiguous messages about which the prior is informative.

The literal speaker's choice in (\ref{s:level0}) is regulated by a soft-max parameter $\lambda$, $\lambda \geq 1$ \citep{luce:1959,sutton+barto:1998}. As $\alpha$ increases, choices made in production are more rational; higher values lead to more deterministic expected utility maximizing behavior.

Pragmatic behavior resembles its literal counterpart. As described above, the crucial difference is that level $n+1$ speakers/hearers reason about level $n$ hearer/speaker behavior. That is, they reason about how a rational level $n$ interlocutor would use or interpret a message and behave according to these expectations. Additionally, pragmatic production is further regulated by a parameter $\alpha$ which controls the tension between semantics and pragmatics, $\alpha \in (0,1]$. Lower values lead to more literal production, whereas higher values lead to more pragmatic behavior. 

The combination of a lexicon with its use, i.e., a degree of pragmatic sophistication, yields a type $t \in T$. Accordingly, $T$ contains all all lexica and reasoning level pairings under consideration. Types are the basic units on which our population dynamics operate. 

\subsection{Replication \& expressivity}
On a population level, expressiveness, or communicative efficiency, has received particular attention from investigations using evolutionary game theory \citep{nowak+krakauer:1999,nowak+etal:2000, nowak+etal:2002}. Under this view, successful communication confers a higher fitness to some types relative to less successful ones. As a consequence, types with a higher fitness replicate and spread through the population, whereas the proportion of communicatively less efficient types decreases. This association of a type's communicative success within a population with changes in the types present in it creates a feedback loop that pressures the population towards greater expressivity. The replicator equation gives us the means to make these dynamics precise.

The proportion of types in a given population is captured by a vector $x$, where $x_i$ is type $i$'s proportion in the population. The fitness of a type $i$, $f_i$, is given by its expected utility when interacting in this population. That is, its fitness is the sum of its weighted expected communicative success, $f_i = \sum_j x_j \text{EU}(t_i,t_j)$. The expected utility of $i$ and $j$ is obtained by considering the expected utility of speaker $i$ interacting with hearer $j$, and vice versa: $\text{EU}(t_i,t_j) = [U_S(t_i,t_j) + U_R(t_i,t_j)]/2$. $U_S(x,y)$ and $U_R(x,y)$ are respectively $\sum_s P(s)\sum_m S_n(m|s;L) \sum_{s'} R_o(s'|m;L) \delta(s,s')$ and $U_S(y,x)$ for $n$ and $o$ being the reasoning level of $x$ and $y$, and $\delta(s,s') = 1$ iff $s = s'$ and $0$ otherwise.\footnote{Note that the definition of $U_R(\cdot,\cdot)$ implies equal sender and receiver payoff in an interaction. This need not be so in the general case but suffices for our application.} Lastly, the average fitness of the population is captured by $\Phi$, $\Phi = \sum_i x_i f_i$. This term serves as a normalizing constant for the (discrete) replicator equation; $\dot{x}_i = \frac{x_i f_i}{\Phi}$ 

Under its biological interpretation, the replicator equation captures the idea of fitness-relative selection whereby fitter types produce more offspring, leading to their propagation in subsequent generations. In analogy to biological replication, many aspects of natural language are subject to processees of change across varied time-spans. For example, the replicator equation can be understood as a learning across generations as e.g. in \citealt{nowak+etal:2002}, but also as a process of horizontal adaptation (see \citealt[\S3.3]{benz+etal:2005b} for discussion). In the following, we take the latter view and assume that interlocutors adapt their language's semantics and how it is used to that which works best within their population. In contrast to the effect of language acquisition from a generation to the next, this means that expressivity exherts its pressure on the current members of a linguistic community. 

Nowak \& et al. did not only consider replication, but also recognized the relevance of variation introduced by a language's transmition across generations, construed as mutation. As a consequence, the offspring of a type may end up adopting a different type than their parents. However, in this work mutation rates were construed as independent from a type. That is, the variation introduced by generational turnovers did not depend on a the relative learnability of a type based on learning data or learning biases. Therefore, to address the issue of selecting a particular type over (near) functional equivalents, we turn to a different strand of research in cultural evolution: {\em iterated learning}. 

\subsection{Mutation \& learning}
Iterated learning is a process in which the behavior of one individual serves as learning input for another, who's behavior subsequently serves as input for a new learner, and so on. In linguistic terms, this process can be construed as chains of parents and children, where the parent produces linguistic data from which the child infers a language. The latter then goes on to produce linguistic data for a new generation of na\"ive learners. A central result of this paradigm is that this process leads to simpler and more regular languages in virtue of the pressure towards greater learnability exherted (surveys of experimental data and models are given in \citealt{kirby+etal:2014} and \citealt{tamariz+kirby:2016}).

\hl{Some more technical exposition of IL. Mainly based on G+K07}

In our present setup, the learners' task is to perform a joint inference over lexica and signaling behavior. As with the sole replicator dynamics, the mutator dynamics are deterministic. They are codified in a transition matrix $Q$, where $Q_{ij}$ indicates the probability of the children of a parent of type $i$ adopting type $j$. This, in turn, is proportional to the probability of $i$ producing the learning data and that of $j$ given the data. 

The elements of the set of learning data $D$ are sequences of length $k$ of state-message pairings. That is, a sequence of observations of language use. Put differently, a datum $d \in D$ contains $k$ members of the set $\{\tuple{s_i,m_j} | s_i \in S, m_j \in M\}$ and $D$ is the set of all such sequences. Having fixed $D$,
\[
 Q_{ij} \propto \sum_{d \in D} P(d|t_i) F(t_j,d),
\]
where $F(t_j,d) \propto P(t_j|d)^l$ and $P(t_j|d) \propto P(t_j) P(d|t_j)$. Given a type $i$, $P(d|t_i)$ can be straightforwardly computed based on $t_i$'s production behavior. However, the above involves two crucial aspects that require a more detailed discussion: the posterior parameter $l$ and the learning prior $P \in \Delta(\mathcal{T})$. 

\paragraph{Posterior parametrization.}

Crucially, the posterior is parametrized by $l$, $l \geq 1$. When $l = 1$ learners sample from the posterior. As $l$ increases towards infinity, the learners tendency to posterior maximization increases. \hl{Some more discussion on posterior maximization}

\paragraph{Learning prior.} 



 

\subsection{Summary}
Following past research, we have argued for expressivity, learnability and simplicity as central pressures that apply on language change. The dynamics we propose model them as fitness-relative replication, mutation based on iterated Bayesian learning, and a prior that biases learners for compressed lexical meanings. Taken together the dynamics are described by the replicator-mutator dynamics \citep{hofbauer+sigmund:2003}: 

\[ 
\hat{x_i} = \sum_j Q_{ji} \frac{x_jf_j}{\Phi}
\]

The basic units that these pressures apply to are a combination of semantics, i.e., a lexicon, and its use. The expressivity of these types depends on their communicative efficiency within a population. Their learnability depends on the fidelity by which they can be infered by a new generation of learners based on production data. In particular, learners perform a joint inference on types of linguistic behavior and lexical meaning. This process is influenced by the learners' preference for simpler lexical representations, codified as a learning bias.

\hl{Summarize novelties \& advantages
1. Advantage of connection between two strands of research
2. Simple and general mean field dynamics}

%\hl{So far, we have focused on highlighting the strands of research and assumptions present in the model. Less has been said about its departure from past approaches, beyond the virtues we argued their synthesis gives rise to. Given the different foci of these investigations, most comparisons on a general level are misplaced. However, \citet{kirby+etal:2015} ...}






%%%Differences to our earlier CogSci proceedings paper:
%Sequences and atomic observations. Before, the set of all observations was $O =$\linebreak  $\{\tuple{\tuple{s_1,m_i},\tuple{s_2,m_j}} | m_i, m_j \in M\}$. A member of $O$ encodes that a teacher produced $m_i$ in state $s_1$ and $m_j$ in $s_2$, i.e., it encodes one witnessed message for each state.

%Observations as production. Instead of taking the space of all possible sequences of length $k$ into consideration, we take sample from $O$ $k$-times according to the production probabilities of each type; $P(o = \tuple{s,m} | t_i) = P(s) P(m|s,t_i)$. $n$ such $k$-length sequences are sampled for each type. As a consequence, the data used for computing $Q_i$ is not the same as that used for $j$ $(i \neq j)$.

%No communal learniner; only parental
%%%

\section{Lack of semantic upper-bounds in lexical meaning}
We set out to investigate the prevalence of lexical meanings that allow for regular pragmatic enrichments over other potential alternatives. A particularly well-studied type of conventional pragmatic enrichment are so-called {\em scalar implicatures} (cf. \citealt{horn:1972,gazdar:1979}). These inferences are licensed for groups of expressions ordered in terms of informativity, here understood as an entailment induced order.

For instance, {\em some} is entailed by {\em all}; if it were true that `All students came to class', it would also be true that `Some students came to class'. However, while weaker expressions such as {\em some} are truth-conditionally compatible with stronger alternatives such as {\em all}, this is not necessarily what their use is taken to convey. Instead, the use of a less informative expression when a more informative one could have been used can license a defeasible inference that stronger alternatives do not hold. That is, a hearer who assumes the speaker to be able and willing to provide all relevant information can infer that, since the speaker did not use a stronger alternative ({\em all}), this alternative must not hold. In this way, `Some students came to class' is strengthened as conveying `Some but not all students came to class'. Analogously, a speaker can rely on her interlocutor to draw this inference without having to express this upper-bound overtly, e.g. by stating {\em some but not all}. Put differently, mutual reasoning about rational language use supplies a bound that rules out stronger alternatives pragmatically.

Note that this corresponds to our previous description of the pragmatic use of lexicon $L_a$, repeated below for convenience. A pragmatic hearer who reasons about a speaker's use of message $m_1$ will associate it more strongly with $s_1$ than with $s_2$ given that the latter is unambiguously associated with the latter state. The strength of this association being dependent on their degree of rationality $\lambda$ and their prior over states. Conversely, a pragmatic speaker will reason about her interlocutors interpretation and use the messages accordingly. 

\begin{centering}
$L_a$ = \bordermatrix{~ & m_1 & m_2 \cr 
                  s_1 & 1 & 0 \cr
                  s_2 & 1 & 1 \cr} \hspace{2cm} $L_b$ = \bordermatrix{~ & m_1 & m_2 \cr 
                  s_1 & 1 & 0 \cr
                  s_2 & 0 & 1 \cr}\\[0.5cm]
\end{centering}

We can now rephrase our initial question in terms of scalar implicatures: Why is the lack of lexical upper-bounds in weak scalar alternatives, as $m_1$ in $L_a$, regularly selected for over the alternative of lexicalizing it as in $L_b$? More poignantly, would it not serve language users better if weak(er) expressions such as {\em warm}, {\em or}, {\em some} or {\em big} were truth-conditionally incompatible with stronger alternatives such as, respectively, {\em hot}, {\em and}, {\em all} and {\em huge}?  This question is particularly striking considering the number of classes of expressions that license such inferences across languages (\citealt{horn:1972}, \citealt[252-267]{horn:1984}, \citealt{traugott:2004,vdAuwera:2010}). In what follows we investigate the hypothesis that this preference is driven by the advantage in compression that lexical meanings lacking an upper-bound have over those that explicitly codify it. 

\hl{Other alternative explanations}


\subsection{Application}




\paragraph{Procedural description.} The game is initialized with some arbitrary distribution over player types. At the game's onset we compute $Q$ once based on the sets  of sequences $D$ (one for each parent type). Replicator dynamics are computed based on the fitness of each type in the current population as usual. $Q$ is computed anew for each independent run (of $g$ generations) given that it depends on $D$, which is sampled from production probabilities.


\paragraph{Languages.} We consider a population of players with two signaling behaviors, literal and Gricean (level $0$ and $1$ below), each equipped with one of $6$ lexicons. This yields a total of $12$ distinct player types $t \in T$. $|M| = |S| = 2$, i.e., a lexicon is a $(2,2)$-matrix. These are listed in Table \ref{tab:lexica}. 

\begin{table}[h]
\centering 
\begin{tabular}{l c l}
$L_1$ = $\begin{pmatrix} 0 & 0 \\ 1 & 1 \end{pmatrix}$ & 
$L_2$ = $\begin{pmatrix} 1 & 1 \\ 0 & 0 \end{pmatrix}$ & 
$L_3$ = $\begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$\\[0.5cm]

$L_4$ = $\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$ &
$L_5$ = $\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}$ &
$L_6$ = $\begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}$
\end{tabular}
\caption{{\footnotesize Set of considered lexica.}}
\label{tab:lexica}
\end{table}

As in the CogSci paper, $L_4$ (semantic upper-bound for $m_2$) and $L_5$ (no semantic upper-bound for $m_2$) are the target lexica. Gricean $L_5$ users can convey/infer the bound pragmatically, while literal/Gricean $L_4$ users do so semantically.


\subsection{Model parameters \& procedure} 
\begin{enumerate}
  \item Sequence length $k$
  \item Pragmatic production parameter $\alpha$
  \item Rationality parameter $\lambda$
  \item Learning prior over types (lexica); cost parameter $c$. $p^\star(t_i) \propto n - c \cdot r$ where $n$ is the total number of states and $r$ that of upper-bounded messages only true of $s_1$ in $t_i$'s lexicon (if only $s_1$ is true of a message, then this message encodes an upper-bound). Then the score for $L_1$, $L_3$, $L_5$ is $2$, that of $L_4$ and $L_6$ is $2-c$, and that of $L_2$ is $2-2c$; Normalization over lexica scores yields the prior over lexica (which is equal to the prior over types).   
  \item Prior over meanings ($pr$). We assume that $pr(s) = \frac{1}{|S|}$ for all $s$.
  \item True state distribution ($P$). We currently assume that $P = \frac{1}{|S|}$ but it may be interesting to vary this
  \item Learning parameter $l \geq 1$ with $1$ corresponding to probability matching, and MAP as $l$ approaches infinity
  \item $n$ is the sample of sequences of observations of length $k$ sampled from the production probabilities of each type
  \item Number of generations $g$
\end{enumerate}




\section{Discussion}

\section{Extensions}
\subparagraph{(I) Cost for pragmatic reasoning.} At least in the CogSci setup the effect of adding cost to pragmatic reasoning is unsurprising: High cost for pragmatic signaling lowers the prevalence of pragmatic types. Lexica that semantically encode an upper-bound benefit the most from this. However, the cost needed to be substantial to make the pragmatic English-like lexicon stop being the incumbent type (particularly when learning is communal). 

\subparagraph{(II) Negative learning bias.} Instead of penalizing complex semantics (semantic upper-bounds) one may consider penalizing simple semantics (no upper-bounds). This is useful as a sanity check but also yields unsurprising results in the CogSci setup: The more learners are biased against simple semantics, the more prevalent are lexica that semantically encode upper-bounds. 

\subparagraph{(III) Inductive bias.} A second learning bias that codifies the idea that lexica should be uniform, i.e. be biased towards either lexicalizing an upper-bound for all weaker alternatives in a scalar pair or for none.

\subparagraph{(IV)  Uncertainty.} The other advantage of non-upper bounded semantics lies in being non-committal to the negation of stronger alternatives when the speaker is uncertain. Adding this to the model requires the most changes to our present setup and some additional assumptions about the cues available to players to discern the speaker's knowledge about the state she is in. 

\subparagraph{(V) More scalar pairs.} Taking into consideration more than one scalar pair. Preliminary results suggest that this does not influence the results in any meaningful way without further additions, e.g. by (III).

\subparagraph{(VI) More lexica.} Not necessary. Preliminary results suggest that considering more lexica has no noteworthy effect on the dynamics (tested with all possible 2x2 lexica).

\subparagraph{(VII) State frequencies.} Variations on state frequencies. This may have an interesting interaction with (III).

\subparagraph{(VIII) Reintroduction of communal learning.} One possibility: The probably $N_{ij}$ with which a child of $t_i$ adopts $t_j$ could be the weighted sum of $Q_{ij}$ (as before) and a vector we get from learning from all of the population: $L_j = \sum_d P(d | \vec{p})  P(t_j | d)$, where $P(d | \vec{p}) = \sum_{i} P(d | t_i)  \vec{p}_i$ is the probability of observing $d$ when learning from a random member of the present population distribution.

\section{Conclusion}

%%% Old snippets %%%
%\section{Conveying upper-bounds}
%Scalar inferences refer to the pragmatic derivation of an upper-bound for weak scalar expressions to the effect that stronger alternatives are inferred to not hold, e.g. {\em some students came} may be taken to convey that {\em not all students came}. The order of an expression such as {\em some} with respect to an alternative, e.g., {\em all}, is induced by entailment. For instance, {\em all students came} entails {\em most students came}, which in turn entails {\em some students came}. In this sense, {\em some} is weaker than {\em all}. A considerable class of natural language expressions do not lexicalize an upper-bound and can be ordered in this fashion, allowing for their pragmatic strengthening. As alluded at above, examples in English include numerals, scalar adjectives, quantifiers, modals, and connectives. \hl{possibly add some typological data on universality, frequency, monomorphemic status}
%
%The pragmatic enrichment of the semantic content of such expressions is enabled by mutual reasoning \citep{grice:1975}. More specifically, it is driven by interlocutors' mutual expectations of rational language use. The hearer reasons about the speaker's choice of a weak alternative over a stronger one. Had the speaker known that a stronger alternatives holds, she would have said so as this would have been more informative. Since she did not, the hearer can infer that the stronger alternative does not hold. Analogously, a speaker who reasons about her addressee may rely on her to derive this inference. In this way, a strengthened, upper-bounded, state of affairs can be conveyed without codifying the bound explicitly in the semantics.
%
%However, while pragmatics offers means to convey upper-bounds, the question why they are not part of the lexical meaning of these expressions remains. There are two main explanatory venues for this pattern. The first targets the functional advantages a lack of upper-bounds may confer to language users, whereas the second focuses on a learnability advantage of simpler over more complex semantic representations. 
%
%\paragraph{Function-based explanations.} Two assumptions are key to the pragmatic strengthening of weak alternatives: (the assumption of) cooperation and knowledge about the issue at hand. That is, the hearer needs to assume the speaker to be as informative as possible, i.e., not to withhold information, and that the speaker is knowlegeable, e.g., she knows whether {\em all students came}. Conversely, the speaker needs to assume the hearer to regard these conditions as satisfied. It is not difficult to imagine scenarios where either or both of these conditions are not given. For instance, the speaker may (be assumed to) not want to disclose all information about the students' attendance, or may have left early without being able to verify the attendance to a satisfactory degree. 
%
%\hl{Discussion of functional pressures for a lack of upper-bounds}

%\paragraph{Learning-based explanations.}
%
%\hl{Discussion of our main assumption that a lack of upper-bounds provides a learnability advantage framed in terms of relative representational simplicity over the codification of an upper-bound. {\bf Should this be made precise? If so, in which way?}}

%\bibliographystyle{apacite}
\bibliographystyle{unsrtnat}

%\setlength{\bibleftmargin}{.125in}
%\setlength{\bibindent}{-\bibleftmargin}
\bibliography{./bounds-rmd}


\end{document}
